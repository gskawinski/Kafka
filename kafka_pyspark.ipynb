{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nSpark structured streaming provides rich APIs to read from and write to Kafka topics. When reading from Kafka, Kafka sources can be created for both streaming and batch queries. When writing into Kafka, Kafka sinks can be created as destination for both streaming and batch queries too. \\n- https://kontext.tech/article/475/spark-structured-streaming-read-from-and-write-into-kafka-topics\\n\\nSpark SQL for Kafka is not built into Spark binary distribution. Thus you need to ensure the following jar package is included into Spark lib search path or passed when you submit Spark applications. \\n\\nAn easier way is to use --packages option when deploy application:\\n\\nspark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0  spark-kafka.py\\n\\nSpark will automatically download the artifact and also its dependencies automatically from local repositories or configured remote repositories. \\n\\nKAFKA and MONGODB\\nhttps://kontext.tech/article/841/spark-read-and-write-data-with-mongodb\\n\\nKAFKA and SQL\\nhttps://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark\\nhttps://kontext.tech/article/1061/pyspark-read-data-from-mariadb-database\\n\\n\\nDELTA LAKE TABLE\\nhttps://kontext.tech/article/1175/introduction-to-delta-lake-with-pyspark\\nhttps://kontext.tech/code/1186/time-travel-with-delta-table-in-pyspark\\n\\nA LOT OF ART\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Spark structured streaming provides rich APIs to read from and write to Kafka topics. When reading from Kafka, Kafka sources can be created for both streaming and batch queries. When writing into Kafka, Kafka sinks can be created as destination for both streaming and batch queries too. \n",
    "- https://kontext.tech/article/475/spark-structured-streaming-read-from-and-write-into-kafka-topics\n",
    "- https://urlit.me/blog/pyspark-structured-streaming-read-from-files/\n",
    "\n",
    "Spark SQL for Kafka is not built into Spark binary distribution. Thus you need to ensure the following jar package is included into Spark lib search path or passed when you submit Spark applications. \n",
    "\n",
    "An easier way is to use --packages option when deploy application:\n",
    "\n",
    "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0  spark-kafka.py\n",
    "\n",
    "Spark will automatically download the artifact and also its dependencies automatically from local repositories or configured remote repositories. \n",
    "\n",
    "KAFKA and MONGODB\n",
    "https://kontext.tech/article/841/spark-read-and-write-data-with-mongodb\n",
    "\n",
    "KAFKA and SQL\n",
    "https://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark\n",
    "https://kontext.tech/article/1061/pyspark-read-data-from-mariadb-database\n",
    "\n",
    "\n",
    "DELTA LAKE TABLE\n",
    "https://kontext.tech/article/1175/introduction-to-delta-lake-with-pyspark\n",
    "https://kontext.tech/code/1186/time-travel-with-delta-table-in-pyspark\n",
    "\n",
    "A LOT OF ART\n",
    "https://kontext.tech/tag/pyspark?p=6\n",
    "\n",
    "\n",
    "SPARK STREAMING DOCS\n",
    "- https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\n",
    "- https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kafka source for batch queries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "appName = \"KafkaStream\"\n",
    "master = \"local\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(master) \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# kafka parameters\n",
    "kafka_server = \"localhost:9092\"\n",
    "kafka_topic =\"gjson\"\n",
    "\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", kafka_topic ) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "''' \n",
    "Data is posted in Kafka in the following dataframe format with some additional columns. \n",
    "The value column contains the actual data in Binary format.\n",
    "'''\n",
    "\n",
    "# Get stream Schema\n",
    "df.printSchema()\n",
    "\n",
    "# converts key and value columns to STRING data type from binary. \n",
    "df = df.withColumn('key_str', df['key'].cast('string').alias('key_str')).drop('key') \\\n",
    "        .withColumn('value_str', df['value'].cast('string').alias('key_str')).drop('value')\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "streaming_df = df\n",
    "\n",
    "# Lets read the data from value column, cast to string and expand the JSON\n",
    "# Parse value from binay to string\n",
    "json_df = streaming_df.selectExpr(\"cast(value as string) as value\")\n",
    "\n",
    "# Create some schema for the Value object cominig from kafka stream - value\n",
    "\n",
    "# JSON Schema\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "\n",
    "# example schema for temp sensor\n",
    "json_schema = StructType([StructField('customerId', StringType(), True), \\\n",
    "StructField('data', StructType([StructField('devices', ArrayType(StructType([ \\\n",
    "StructField('deviceId', StringType(), True), \\\n",
    "StructField('measure', StringType(), True), \\\n",
    "StructField('status', StringType(), True), \\\n",
    "StructField('temperature', LongType(), True)]), True), True)]), True), \\\n",
    "StructField('eventId', StringType(), True), \\\n",
    "StructField('eventOffset', LongType(), True), \\\n",
    "StructField('eventPublisher', StringType(), True), \\\n",
    "StructField('eventTime', StringType(), True)])\n",
    "\n",
    "# Define the JSON structure for Poem   { 'type': poem_type, 'message': poem_message }\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Apply Schema to JSON value column and expand the value\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], json_schema)).select(\"value.*\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to two topics\n",
    "\n",
    "kafka_topic_1 =\"gjson\"\n",
    "kafka_topic_2 =\"gnone\"\n",
    "\n",
    "# Concatenate the two topics into a single string separated by commas. \n",
    "kafka_multi_topics = \",\".join([kafka_topic_1, kafka_topic_2])\n",
    "\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\",  kafka_multi_topics ) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "df = df.withColumn('key_str', df['key'].cast('string').alias('key_str')).drop(\n",
    "    'key').withColumn('value_str', df['value'].cast('string').alias('key_str')).drop('value')\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "# Cast the value column as String (assuming the messages are in String format)\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Perform further transformations or analysis\n",
    "# For example, you can split the value into words\n",
    "words = df.selectExpr(\"value\", \"explode(split(value, ' ')) as word\")\n",
    "\n",
    "# Display the streaming results to the console\n",
    "query = words \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The startingOffsets and endingOffsets options in PySpark Structured Streaming allow you to specify the starting and ending offsets when reading from a Kafka topic.\n",
    "\n",
    "''' \n",
    "\n",
    "startingOffsets and endingOffsets options in PySpark Structured Streaming are used to define the starting and ending offsets when reading from a Kafka topic.\n",
    "\n",
    "Topic kontext-kafka: reading from partition 2 for events starting with offeset 98; partion 0 and 1 for all events. \n",
    "Topic kontext-events: reading from partition 0 for all the events. \n",
    "infoIn the offset json, -2 represents an offset can be used to refer to earliest and -1 to lates\n",
    "\n",
    ".option(\"startingOffsets\", \"\"\"{\"kontext-kafka\":{\"2\":98,\"0\":0,\"1\":0},\"kontext-events\":{\"0\":-2}}\"\"\") \\\n",
    ".option(\"endingOffsets\", \"\"\"{\"kontext-kafka\":{\"2\":-1,\"0\":-1,\"1\":-1},\"kontext-events\":{\"0\":-1}}\"\"\") \\\n",
    "\n",
    "'''\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStructuredStreamingExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_kafka_topic\"\n",
    "\n",
    "# Define starting offsets and ending offsets for Kafka topics\n",
    "starting_offsets = {\n",
    "    \"your_kafka_topic\": {\"0\": 0, \"1\": 0, \"2\": 98},\n",
    "    \"another_kafka_topic\": {\"0\": -2}\n",
    "}\n",
    "\n",
    "# Define starting offsets for single Kafka topic\n",
    "starting_offsets = {\n",
    "    \"your_kafka_topic\": {\"0\": 0, \"1\": 0, \"2\": 98}\n",
    "}\n",
    "\n",
    "# Subscribe to Kafka topics with specified starting offsets\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", str(starting_offsets)) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get information about the Kafka topic, specifically the number of partitions\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStructuredStreamingExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_kafka_topic\"\n",
    "\n",
    "# Get information about the Kafka topic, specifically the number of partitions\n",
    "kafka_partitions = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(partition AS INT)\") \\\n",
    "    .distinct() \\\n",
    "    .collect()\n",
    "\n",
    "# Extract the number of partitions from the result\n",
    "num_partitions = len(kafka_partitions)\n",
    "\n",
    "# Define starting offsets for Kafka topic\n",
    "starting_offsets = {kafka_topic: {str(partition): 0 for partition in range(num_partitions)}}\n",
    "\n",
    "# Define ending offsets for Kafka topic (setting to -1 for continuous streaming)\n",
    "ending_offsets = {kafka_topic: {str(partition): -1 for partition in range(num_partitions)}}\n",
    "\n",
    "# Subscribe to Kafka topic with specified starting and ending offsets for all partitions\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", str(starting_offsets)) \\\n",
    "    .option(\"endingOffsets\", str(ending_offsets)) \\\n",
    "    .load()\n",
    "\n",
    "# Cast the value column as String (assuming the messages are in String format)\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Perform further transformations or analysis\n",
    "# For example, you can split the value into words\n",
    "words = df.selectExpr(\"value\", \"explode(split(value, ' ')) as word\")\n",
    "\n",
    "# Display the streaming results to the console\n",
    "query = words \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to a pattern\n",
    "# subscribe to all topics that match with regular expression 'kontext.*'. It reads all events in all partitions. \n",
    "# Option subscribePattern is used to match against topic names. \n",
    "\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribePattern\", \"kontext.*\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Kafka source for streaming queries\n",
    "To read from Kafka for streaming queries, we can use function SparkSession.readStream. Kafka server addresses and topic names are required.  Spark can subscribe to one or more topics and wildcards can be used to match with multiple topic names\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "# Parse value from binay to string\n",
    "json_df =  df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Extract key and value columns from the Kafka message\n",
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "\n",
    "''' \n",
    "We cannot directly show the above data frame as it uses a streaming source. The following exception will occur if you call df.show():\n",
    "pyspark.sql.utils.AnalysisException: Queries with streaming sources must be executed with writeStream.start()\n",
    "\n",
    "so use this data frame to write into another different topic. \n",
    "\n",
    "'''\n",
    "\n",
    "# Write Spark data frame to Kafka topic\n",
    "# For streaming sourced data frame, we can directly use DataFrame.writeStream function to write into a Kafka topic.\n",
    "\n",
    "# Display the data in the console\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "    # .option(\"topic\", \"topic-kafka-3\") \\\n",
    "    # .option(\"checkpointLocation\", \"file:///F:/tmp/topic-kafka/checkpoint\") \\\n",
    "    # .start()\n",
    "\n",
    "spark.streams.awaitAnyTermination()\n",
    "\n",
    "# Wait for the stream to finish\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "StreamingContext is the main entry point for creating and managing a Spark Streaming application in PySpark. It represents the connection to a Spark cluster and can be used to create DStreams (Discretized Streams), which are the fundamental abstraction in Spark Streaming for processing live data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "In Apache Spark Streaming, a checkpoint is a mechanism to provide fault-tolerance and recovery of streaming applications. It involves saving the state of the streaming application to a reliable distributed file system (like HDFS) periodically. This checkpoint information includes the metadata and data necessary for recovery in case of a failure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StreamingContextExample\").getOrCreate()\n",
    "\n",
    "# Create a StreamingContext with a batch interval of 5 seconds\n",
    "# The batch interval determines the duration of each micro-batch, which is the unit of data processing in Spark Streaming.\n",
    "ssc = StreamingContext(spark.sparkContext, batchDuration=5)\n",
    "\n",
    "# Checkpoint directory is set for fault-tolerance\n",
    "ssc.checkpoint(\"/path/to/checkpoint_directory\")\n",
    "\n",
    "\n",
    "# Define DStream from a socket source , # Define the input DStream by connecting to a socket (for example)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "\n",
    "# Perform a simple transformation (e.g., word count)\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")).countByValue()\n",
    "\n",
    "# Print the results to the console\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
