{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/confluentinc/confluent-kafka-python/tree/master/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from confluent_kafka.admin import AdminClient, NewTopic, ConfigResource\n",
    "from confluent_kafka import Producer, Consumer, KafkaError, TopicCollection, KafkaException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@mrugankray/create-avro-producer-for-kafka-using-python-f9029d9b2802\n",
    "# https://www.stackstalk.com/2022/08/avro-producer-consumer-python.html\n",
    "# https://stackoverflow.com/questions/61509939/how-to-programatically-register-avro-schema-in-kafka-schema-registry-using-pytho\n",
    "# https://betterprogramming.pub/avro-producer-with-python-and-confluent-kafka-library-4a1a2ed91a24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up AdminClient for topic management\n",
    "config = { \n",
    "    \"bootstrap.servers\": \"localhost:9092\"\n",
    "}\n",
    "admin_client = AdminClient(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_exists(admin, topic):\n",
    "    \n",
    "    metadata = admin.list_topics()\n",
    "    for t in iter(metadata.topics.values()):\n",
    "        if t.topic == topic:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "topic_name = 'greg'\n",
    "if topic_exists(admin_client, topic_name):\n",
    "    print(f\"Topic '{topic_name}' exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 'gregis' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# create new topic and return results dictionary\n",
    "def create_topic(admin, topic, partitions = 1, replicas=1):\n",
    "\n",
    "    if topic_exists(admin, topic):\n",
    "        print(f\"Topic '{topic}' already exists.\")\n",
    "        return\n",
    "\n",
    "    # Create a new topic with parameters\n",
    "    new_topic = NewTopic(topic, num_partitions=partitions, replication_factor=replicas) \n",
    "    result_dict = admin.create_topics([new_topic])\n",
    "\n",
    "    for topic, future in result_dict.items():\n",
    "        try:\n",
    "            future.result()  # The result itself is None\n",
    "            #print(\"Topic {} created\".format(topic))\n",
    "            print(f\"Topic '{topic}' created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create topic {}: {}\".format(topic, e))\n",
    "\n",
    "# Create topic if it doesn't exist\n",
    "\n",
    "topic_name = 'gregis'\n",
    "\n",
    "if not topic_exists(admin_client, topic_name):\n",
    "    create_topic(admin_client, topic_name)\n",
    "else:\n",
    "    print(f\"Topic '{topic_name}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Describe the New Kafka Topic\n",
    "# get max.message.bytes property\n",
    "def get_topic_config(admin, topic):\n",
    "    resource = ConfigResource('topic', topic)\n",
    "    result_dict = admin.describe_configs([resource])\n",
    "    config_entries = result_dict[resource].result()\n",
    "    pprint(config_entries)\n",
    "    max_size = config_entries['max.message.bytes']\n",
    "    return max_size.value\n",
    "\n",
    "current_max = get_topic_config(admin_client, topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic gregis deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def delete_topic(admin_client, topic_name):\n",
    "    \"\"\"\n",
    "    Delete a Kafka topic.(asynchronously)\n",
    "    Parameters:\n",
    "    - admin_client: Kafka AdminClient instance.\n",
    "    - topic_name: Name of the topic to be deleted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the topic exists\n",
    "    if not topic_exists(admin_client, topic_name):\n",
    "        print(f\"Topic '{topic_name}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Delete the topic\n",
    "    fs = admin_client.delete_topics([topic_name], operation_timeout=20.0)\n",
    "    # Returns a dict of <topic,future>.\n",
    "\n",
    "    # Wait for operation to finish.\n",
    "    for topic, f in fs.items():\n",
    "        try:\n",
    "            f.result()  # The result itself is None\n",
    "            print(\"Topic {} deleted\".format(topic))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to delete topic {}: {}\".format(topic, e))\n",
    "\n",
    "\n",
    "delete_topic(admin_client, topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "# Create an AdminClient instance\n",
    "admin_client = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "def describe_topics(a, topic_names):\n",
    "    \"\"\"\n",
    "    Describe a Kafka topic and return information in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - admin_client: Kafka AdminClient instance.\n",
    "    - topic_name: Name of the topic to be described.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing information about the topic.\n",
    "    \"\"\"\n",
    "\n",
    "    topics = TopicCollection(topic_names)\n",
    "    futureMap = a.describe_topics(topics, request_timeout=10)\n",
    "\n",
    "    for topic_name, future in futureMap.items():\n",
    "        try:\n",
    "            # Extract information from topic metadata\n",
    "            print(\"Topic Information:\")\n",
    "            t = future.result()\n",
    "            print(\"Topic name             : {}\".format(t.name))\n",
    "            print(\"Topic id               : {}\".format(t.topic_id))\n",
    "            print(\"Partitions             : {}\".format(len(t.partitions)))\n",
    "            #print(f\"Partitions: {len(t.partitions)}\")\n",
    "            #print(f\"Replication Factor: {t.replication_factor}\")\n",
    "            print(\"Partition Information\")\n",
    "            for partition in t.partitions:\n",
    "                #print(\"    Id                : {}\".format(partition.id))\n",
    "                leader = partition.leader\n",
    "                print(\"    Replicas          : {}\".format(len(partition.replicas)))\n",
    "                print(f\"    Leader            : {leader}\")\n",
    "            print(\"\")\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Error while describing topic '{}': {}\".format(topic_name, e))\n",
    "            raise\n",
    "\n",
    "topic_name = [ 'greg']\n",
    "describe_topics(admin_client, topic_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producer Delivery Callback\n",
    "def callback_report(err, event):\n",
    "    \"\"\"\n",
    "    Reports the success or failure of a message delivery.\n",
    "    Args:\n",
    "        err (KafkaError): The error that occurred on None on success.\n",
    "        msg (Message): The message that was produced or failed.\n",
    "    \"\"\"\n",
    "    if err:\n",
    "        print(f'Produce to topic {event.topic()} failed for event: {event.key()}')\n",
    "        return\n",
    "    else:\n",
    "        max_length = 10\n",
    "\n",
    "        val = event.value().decode('utf8')[:max_length] \n",
    "        key = event.key().decode('utf8')\n",
    "        print(f'Messge: {val}, with Key: {key} produced to Topic:{event.topic()}  Part[{event.partition()}] at offset {event.offset()}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# random Poem generator/iterantor \n",
    "def poem_generator():\n",
    "    fake = Faker()\n",
    "    \n",
    "    poem_types = ['Haiku', 'Sonnet', 'FreeVerse', 'Limerick']\n",
    "    \n",
    "    while True:\n",
    "        poem_type = random.choice(poem_types)\n",
    "        poem_message = fake.sentence(nb_words=random.randint(5, 15))\n",
    "        \n",
    "        yield {\n",
    "            'type': poem_type,\n",
    "            'message': poem_message\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    poems = poem_generator()\n",
    "\n",
    "    for _ in range(5):  # Generate and print 5 poems\n",
    "        poem = next(poems)\n",
    "        print(f\"Type: {poem['type']}, Poem: {poem['message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_topic(producer, topic_name, message, key):\n",
    "    \"\"\"\n",
    "    Produce a message to a specified Kafka topic using a provided Kafka producer instance.\n",
    "    Parameters:\n",
    "    - producer (confluent_kafka.Producer): The Kafka producer instance.\n",
    "    - topic_name (str): The name of the Kafka topic to which the message will be sent.\n",
    "    - message (str): The message content to be sent to the Kafka topic.\n",
    "    - key (str): The key associated with the Kafka message.\n",
    "    Returns: None\n",
    "    Usage Example:\n",
    "    write_to_topic(producer, \"example_topic\", \"Hello, Kafka!\", key=\"example_key\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Produce a message to the specified topic\n",
    "        producer.produce(topic=topic_name, value=message, key=key, callback=callback_report)\n",
    "        producer.flush()\n",
    "    except Exception as e:\n",
    "        # Handle exceptions raised during message production\n",
    "        print(f\"Error during message production: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer, Consumer, KafkaError, TopicCollection, KafkaException\n",
    "import time\n",
    "\n",
    "# Create Producer instance\n",
    "# Set up Producer for producing messages\n",
    "producer = Producer({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "# Kafka topic name\n",
    "topic_name = \"greg\"\n",
    "\n",
    "# Poem generator\n",
    "poems = poem_generator()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        poem = next(poems)\n",
    "        print(f\"Type: {poem['type']}, Poem: {poem['message']}\")\n",
    "        \n",
    "        # Write to Kafka topic\n",
    "        write_to_topic(producer, topic_name, poem['message'], key=poem['type'])\n",
    "        \n",
    "        # Introduce a delay between 1 to 4 seconds\n",
    "        time.sleep(random.uniform(1, 4))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Handle Ctrl+C gracefully\n",
    "    print(\"Stopping Kafka producer.\")\n",
    "finally:\n",
    "    # Close the Kafka producer\n",
    "    producer.flush()\n",
    "    producer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign topic partitions to consumer\n",
    "def assignment_callback(consumer, partitions):\n",
    "    \"\"\" \n",
    "    Callback function invoked when a Kafka consumer is assigned to partitions.\n",
    "    Parameters:\n",
    "    - consumer (confluent_kafka.Consumer): The Kafka consumer instance.\n",
    "    - partitions (list): List of assigned partitions.\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    for p in partitions:\n",
    "        print(f'Assigned to {p.topic}, partition {p.partition}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Consumer for consuming messages\n",
    "consumer = Consumer({\n",
    "        \"bootstrap.servers\": \"localhost:9092\",\n",
    "        \"group.id\": \"my-group\",\n",
    "        \"auto.offset.reset\": \"earliest\"\n",
    "    })\n",
    "topic_to_read = 'greg'\n",
    "consumer.subscribe([topic_to_read], on_assign=assignment_callback)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        event = consumer.poll(1.0)\n",
    "        if event is None:\n",
    "            continue\n",
    "        if event.error():\n",
    "            raise KafkaException(event.error())\n",
    "        else:\n",
    "            val = event.value().decode('utf8')\n",
    "            key = event.key().decode('utf8')\n",
    "            partition = event.partition()\n",
    "            offset = event.offset()\n",
    "            print(f'Received: {val} from partition [{partition}] at offset {offset}     ')\n",
    "            # consumer.commit(event)\n",
    "except KeyboardInterrupt:\n",
    "    print('Canceled by user.')\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized Poem: {\"type\": \"Sonnet\", \"message\": \"Shall I compare thee to a summer's day?\"}\n",
      "Deserialized Poem: {'type': 'Sonnet', 'message': \"Shall I compare thee to a summer's day?\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def serialize_poem(poem_dict):\n",
    "    \"\"\"\n",
    "    Serialize a poem dictionary to a JSON-formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    - poem_dict (dict): The dictionary representing a poem.\n",
    "\n",
    "    Returns:\n",
    "    str: JSON-formatted string.\n",
    "    \"\"\"\n",
    "    return json.dumps(poem_dict)\n",
    "\n",
    "def deserialize_poem(json_string):\n",
    "    \"\"\"\n",
    "    Deserialize a JSON-formatted string to a poem dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - json_string (str): JSON-formatted string.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary representing a poem.\n",
    "    \"\"\"\n",
    "    return json.loads(json_string)\n",
    "\n",
    "# Example usage:\n",
    "poem = {\n",
    "    'type': 'Sonnet',\n",
    "    'message': 'Shall I compare thee to a summer\\'s day?'\n",
    "}\n",
    "\n",
    "# Serialize the poem dictionary to a JSON-formatted string\n",
    "serialized_poem = serialize_poem(poem)\n",
    "print(\"Serialized Poem:\", serialized_poem)\n",
    "\n",
    "# Deserialize the JSON-formatted string back to a poem dictionary\n",
    "deserialized_poem = deserialize_poem(serialized_poem)\n",
    "print(\"Deserialized Poem:\", deserialized_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "In Kafka, serialization and deserialization are crucial for converting data between the internal binary format used by Kafka and the native format of your data. In the case of Python and Kafka, you often need to serialize your data before sending it to Kafka and deserialize it after receiving it. For simplicity, I'll use JSON as the serialization format.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messge: {\"type\": \", with Key: Sonnet produced to Topic:greg  Part[0] at offset 105.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BASIC SERIALIZATION using json.dumps(poem)\n",
    "\n",
    "import json\n",
    "\n",
    "def serialize_poem(poem_dict):\n",
    "    \"\"\"\n",
    "    Serialize a poem dictionary to a JSON-formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    - poem_dict (dict): The dictionary representing a poem.\n",
    "\n",
    "    Returns:\n",
    "    str: JSON-formatted string.\n",
    "    \"\"\"\n",
    "    return json.dumps(poem_dict)\n",
    "\n",
    "# Kafka Procucer\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "\n",
    "# Configure Kafka Producer\n",
    "producer_config = {'bootstrap.servers': 'localhost:9092'}\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "# Poem dictionary\n",
    "poem = {\n",
    "    'type': 'Sonnet',\n",
    "    'message': 'Shall I compare thee to a summer\\'s day?'\n",
    "}\n",
    "\n",
    "# Kafka topic name\n",
    "topic_name = \"greg\"\n",
    "\n",
    "# Serialize and produce the message using JSON \n",
    "serialized_poem = serialize_poem(poem)\n",
    "#serialized_poem = json.dumps(poem)\n",
    "\n",
    "producer.produce(topic_name, key=poem['type'], value=serialized_poem, callback=callback_report)\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned to greg, partition 0\n",
      "Received Poem: {'type': 'Sonnet', 'message': \"Shall I compare thee to a summer's day?\"}\n",
      "Received Poem: {'type': 'Sonnet', 'message': \"Shall I compare thee to a summer's day?\"}\n"
     ]
    }
   ],
   "source": [
    "# BASIC DE-SERIALIZATION using json.dumps(poem)\n",
    "\n",
    "def deserialize_poem(json_string):\n",
    "    \"\"\"\n",
    "    Deserialize a JSON-formatted string to a poem dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - json_string (str): JSON-formatted string.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary representing a poem.\n",
    "    \"\"\"\n",
    "    return json.loads(json_string)\n",
    "\n",
    "# Assign topic partitions to consumer\n",
    "def assignment_callback(consumer, partitions):\n",
    "    \"\"\" \n",
    "    Callback function invoked when a Kafka consumer is assigned to partitions.\n",
    "    Parameters:\n",
    "    - consumer (confluent_kafka.Consumer): The Kafka consumer instance.\n",
    "    - partitions (list): List of assigned partitions.\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    for p in partitions:\n",
    "        print(f'Assigned to {p.topic}, partition {p.partition}')\n",
    "\n",
    "# Kafka Consumer\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "import json\n",
    "\n",
    "# Configure Kafka Consumer\n",
    "consumer_config = {\n",
    "        \"bootstrap.servers\": \"localhost:9092\",\n",
    "        \"group.id\": \"my-group\",\n",
    "        \"auto.offset.reset\": \"earliest\"\n",
    "    }\n",
    "consumer = Consumer(consumer_config)\n",
    "\n",
    "# Kafka topic name\n",
    "topic_name = \"greg\"\n",
    "\n",
    "# Subscribe to the topic\n",
    "consumer.subscribe([topic_name], on_assign=assignment_callback) \n",
    "\n",
    "# Poll for messages\n",
    "while True:\n",
    "    try: \n",
    "        msg = consumer.poll(1.0)  # Adjust the timeout as needed\n",
    "\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        \n",
    "        else:\n",
    "        # Deserialize the message using JSON\n",
    "            try:\n",
    "                #deserialized_poem = json.loads(msg.value().decode('utf-8'))\n",
    "                deserialized_poem =  deserialize_poem(msg.value().decode('utf-8'))\n",
    "                print(\"Received Poem: {}\".format(deserialized_poem))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"Error decoding JSON: {}\".format(e))\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "# Close the consumer\n",
    "consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERIALIZATION using AVRO \n",
    "\n",
    "from avro import schema\n",
    "from avro.io import DatumWriter, BinaryEncoder\n",
    "from io import BytesIO\n",
    "\n",
    "# Poem dictionary\n",
    "poem_data = {\n",
    "    'type': 'Sonnet',\n",
    "    'message': 'Shall I compare thee to a summer\\'s day?'\n",
    "}\n",
    "\"\"\"\n",
    "Avro Schema for Poem \n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Poem\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"type\", \"type\": \"string\"},\n",
    "    {\"name\": \"message\", \"type\": \"string\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Avro schema for the Poem\n",
    "poem_avro_schema_str = \"\"\"\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Poem\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"type\", \"type\": \"string\"},\n",
    "    {\"name\": \"message\", \"type\": \"string\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "poem_avro_schema = schema.parse(poem_avro_schema_str)\n",
    "\n",
    "# Avro serialization\n",
    "\n",
    "# Create a new BytesIO object named bytes_io, which will be used to hold the Avro-encoded data during serialization.\n",
    "bytes_io = BytesIO()\n",
    "# Create Object with the structure for the serialization process.\n",
    "writer = DatumWriter(poem_avro_schema)\n",
    "# Module that provides functionality to encode data in binary format according to the Avro specification.\n",
    "encoder = BinaryEncoder(bytes_io)\n",
    "# Perform the actual serialization.\n",
    "writer.write(poem_data, encoder)\n",
    "# Retrieve the content of the BytesIO object, which now contains the Avro-encoded binary data produced during serialization.\n",
    "avro_serialized_data = bytes_io.getvalue()\n",
    "\n",
    "print(\"Avro Serialized Data:\")\n",
    "print(avro_serialized_data)\n",
    "\n",
    "# DE-SERIALIZATION using AVRO\n",
    "\n",
    "from avro.io import DatumReader, BinaryDecoder\n",
    "\n",
    "\n",
    "poem_avro_schema = schema.parse(poem_avro_schema_str)\n",
    "\n",
    "# Avro deserialization\n",
    "bytes_io = BytesIO(avro_serialized_data)\n",
    "reader = DatumReader(poem_avro_schema)\n",
    "decoder = BinaryDecoder(bytes_io)\n",
    "poem_deserialized_data = reader.read(decoder)\n",
    "\n",
    "print(\"\\nAvro Deserialized Data:\")\n",
    "print(poem_deserialized_data)\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "BytesIO is a class in the io module that allows you to treat a bytes-like object / in-memory byte buffer (in this case, avro_serialized_data) as a file-like object.\n",
    "\n",
    "DatumReader is a class from the avro.io module that reads Avro-encoded data.\n",
    "DatumWriter is a class from the avro.io module that is responsible for writing Avro-encoded data.\n",
    "\n",
    "Both take the Avro schema (poem_avro_schema) as an argument. The schema is necessary for serialization/deserialization to interpret the binary data correctly.\n",
    "\n",
    "BinaryEncoder is another class from the avro.io module that provides functionality to encode data in binary format according to the Avro specification.\n",
    "\n",
    "BinaryEncoder is created to write the Avro-encoded binary data to the BytesIO object.\n",
    "\n",
    "The writer.write(poem_data, encoder) operation serializes the data.\n",
    "The reader.read(decoder) operation reads the binary data, interprets it based on the provided schema, and produces the deserialized data in the form of a Python dictionary \n",
    "\n",
    "bytes_io.getvalue() retrieves the content of the BytesIO object, which now contains the Avro-encoded binary data produced during serialization.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avro Serialized Data: b\"\\x0cSonnetNShall I compare thee to a summer's day?\"\n",
      "Messge: \fSonnetNSh, with Key: Sonnet produced to Topic:gavro  Part[1] at offset 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SERIALIZATION using AVRO without Schema Registry\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "import avro.schema\n",
    "from avro.io import DatumWriter, BinaryEncoder\n",
    "from io import BytesIO\n",
    "\n",
    "from avro.io import DatumReader, BinaryDecoder\n",
    "from io import BytesIO\n",
    "\n",
    "# Kafka Procucer\n",
    "\n",
    "# Configure Kafka Producer\n",
    "producer_config = {'bootstrap.servers': 'localhost:9092'}\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "# Poem dictionary\n",
    "poem_data = {\n",
    "    'type': 'Sonnet',\n",
    "    'message': 'Shall I compare thee to a summer\\'s day?'\n",
    "}\n",
    "\n",
    "# Avro schema for the Poem\n",
    "poem_schema_str = \"\"\"\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Poem\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"type\", \"type\": \"string\"},\n",
    "    {\"name\": \"message\", \"type\": \"string\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "poem_avro_schema = avro.schema.parse(poem_schema_str)\n",
    "\n",
    "\n",
    "# Avro serialization\n",
    "bytes_io = BytesIO()\n",
    "writer = DatumWriter(poem_avro_schema)\n",
    "encoder = BinaryEncoder(bytes_io)\n",
    "writer.write(poem_data, encoder)\n",
    "avro_serialized_data = bytes_io.getvalue()\n",
    "\n",
    "print(f\"Avro Serialized Data: {bytes_io.getvalue()}\")\n",
    "\n",
    "# Kafka topic name\n",
    "topic_name = \"gavro\"\n",
    "\n",
    "# Produce to Kafka\n",
    "producer.produce(topic=topic_name, key=poem['type'], value=avro_serialized_data, callback=callback_report)\n",
    "\n",
    "# Wait for any outstanding messages to be delivered and delivery reports to be received\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned to gavro, partition 0\n",
      "Assigned to gavro, partition 1\n",
      "Received Poem: {'type': 'Sonnet', 'message': \"Shall I compare thee to a summer's day?\"}\n",
      "Consumer interrupted.\n"
     ]
    }
   ],
   "source": [
    "# DE-SERIALIZATION using AVRO without Schema Registry\n",
    "\n",
    "# Assign topic partitions to consumer\n",
    "def assignment_callback(consumer, partitions):\n",
    "    \"\"\" \n",
    "    Callback function invoked when a Kafka consumer is assigned to partitions.\n",
    "    Parameters:\n",
    "    - consumer (confluent_kafka.Consumer): The Kafka consumer instance.\n",
    "    - partitions (list): List of assigned partitions.\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    for p in partitions:\n",
    "        print(f'Assigned to {p.topic}, partition {p.partition}')\n",
    "\n",
    "\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaException\n",
    "from avro.io import DatumReader, BinaryDecoder\n",
    "from io import BytesIO\n",
    "\n",
    "# Kafka Consumer Configuration\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'avro-consumer',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Poem dictionary\n",
    "poem_data = {\n",
    "    'type': 'Sonnet',\n",
    "    'message': 'Shall I compare thee to a summer\\'s day?'\n",
    "}\n",
    "\n",
    "# Avro schema for the Poem\n",
    "poem_schema_str = \"\"\"\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Poem\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"type\", \"type\": \"string\"},\n",
    "    {\"name\": \"message\", \"type\": \"string\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "poem_avro_schema = avro.schema.parse(poem_schema_str)\n",
    "\n",
    "\n",
    "# Create Kafka Consumer\n",
    "consumer = Consumer(consumer_config)\n",
    "\n",
    "# Kafka topic name\n",
    "topic_name = \"gavro\"\n",
    "\n",
    "# Subscribe to the topic\n",
    "consumer.subscribe([topic_name], on_assign=assignment_callback) \n",
    "\n",
    "# Avro deserialization\n",
    "def deserialize_avro(data, schema):\n",
    "    reader = DatumReader(schema)\n",
    "    bytes_io = BytesIO(data)\n",
    "    decoder = BinaryDecoder(bytes_io)\n",
    "    return reader.read(decoder)\n",
    "\n",
    "\n",
    "# Consume messages\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)  # Adjust the timeout as requird\n",
    "\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaException._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                print(\"Reached end of partition, exiting consumer loop.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Error: {}\".format(msg.error()))\n",
    "                continue\n",
    "\n",
    "        # Avro deserialization\n",
    "        avro_deserialized_data = deserialize_avro(msg.value(), poem_avro_schema)\n",
    "        print(\"Received Poem: {}\".format(avro_deserialized_data))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Consumer interrupted.\")\n",
    "finally:\n",
    "    # Close the consumer\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Schema Registry is a centralized service that manages and stores Avro schemas for Kafka producers and consumers. It ensures that producers and consumers in a Kafka ecosystem agree on a common data format, which is crucial for schema evolution and data compatibility.\n",
    "\n",
    "The SchemaRegistryClient is used to communicate with the Schema Registry service. It is responsible for registering and retrieving Avro schemas, as well as performing schema-related operations.\n",
    "\n",
    "When working with the Schema Registry, it's common to use Avro as the data serialization format. Avro is a binary serialization format that includes schema information with the data. The Schema Registry helps manage and store these Avro schemas.\n",
    "\n",
    "The Schema Registry is typically part of the Confluent Platform.\n",
    "Need to install the entire Confluent Platform, which includes the Schema Registry.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom from_dict function\n",
    "def obj_to_dict(msg):\n",
    "    \"\"\"\n",
    "    Custom function to convert the message object to a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - msg: The message object to be converted.\n",
    "\n",
    "    Returns:\n",
    "    dict: The converted dictionary.\n",
    "    \"\"\"\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON SERIALIZATION using JSONSerializer and Schema Registry\n",
    "\n",
    "from confluent_kafka import Producer, Consumer, KafkaException\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry.json_schema import JSONSerializer\n",
    "import json\n",
    "import avro.schema\n",
    "import time\n",
    "\n",
    "# Avro schema for the poem\n",
    "poem_schema_avro_str = \"\"\"\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Poem\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"type\", \"type\": \"string\"},\n",
    "    {\"name\": \"message\", \"type\": \"string\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Converting an Avro schema to a JSON schema involves transforming the Avro-specific syntax into a JSON schema format.\n",
    "\n",
    "# Json schema for the poem\n",
    "poem_schema_json_str = \"\"\"\n",
    " {\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"type\": \"object\",\n",
    "  \"title\": \"Poem\",\n",
    "  \"description\": \"Poem schema description\",\n",
    "  \"properties\": {\n",
    "    \"type\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"message\": {\n",
    "      \"type\": \"string\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"type\", \"message\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Configure Schema Registry\n",
    "schema_registry_conf = {'url': 'http://localhost:8081'}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "# Configure Kafka Producer\n",
    "producer_config = {'bootstrap.servers': 'localhost:9092'}\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "# STR Serializer \n",
    "string_serializer = StringSerializer('utf_8')\n",
    "\n",
    "# JSON Serializer for the Poem schema\n",
    "json_serializer = JSONSerializer(poem_schema_json_str, schema_registry_client)\n",
    "# Create a JSONSerializer instance with your schema\n",
    "json_serializer = JSONSerializer(poem_schema_json_str, obj_to_dict)\n",
    "\n",
    "\n",
    "# Kafka topic to which you want to send the serialized poem\n",
    "topic_name = 'kreg'\n",
    "\n",
    "# Poem generator\n",
    "poems = poem_generator()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        poem = next(poems)\n",
    "        print(f\"Type: {poem['type']}, Poem: {poem['message']}\")\n",
    "\n",
    "        # Serialize the poem dictionary\n",
    "        serialized_poem = json_serializer(poem, SerializationContext(topic_name, MessageField.VALUE))\n",
    "                    \n",
    "\n",
    "        # Produce the serialized poem to the Kafka topic\n",
    "        # key = string_serializer(str(poem['type'])),\n",
    "        producer.produce(topic=topic_name, value=serialized_poem, key=poem['type'], callback=callback_report)\n",
    "    \n",
    "        \n",
    "        # Introduce a delay between 1 to 4 seconds\n",
    "        time.sleep(random.uniform(1, 4))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Handle Ctrl+C gracefully\n",
    "    print(\"Stopping Kafka producer.\")\n",
    "finally:\n",
    "    # Flush the producer to ensure all messages are sent\n",
    "    producer.flush()\n",
    "    # Close the Kafka producer\n",
    "    producer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how you might use the JSONSerializer to serialize a poem dictionary\n",
    "\n",
    "# Assuming poem dictionary like this\n",
    "poem = {\n",
    "    'type': 'Sonnet',\n",
    "    'message': 'Shall I compare thee to a summer\\'s day?'\n",
    "}\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.serialization import SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry.json_schema import JSONSerializer\n",
    "import json\n",
    "\n",
    "# Define josn schema string\n",
    "poem_schema_json_str = \"\"\"\n",
    " {\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"type\": \"object\",\n",
    "  \"title\": \"Poem\",\n",
    "  \"description\": \"Poem schema description\",\n",
    "  \"properties\": {\n",
    "    \"type\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"message\": {\n",
    "      \"type\": \"string\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"type\", \"message\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create a SchemaRegistryClient instance\n",
    "# if you are running it locally, it might be something like http://localhost:8081.\n",
    "schema_registry_url = \"http://localhost:8081\"   \n",
    "schema_registry_client = SchemaRegistryClient({\"url\": schema_registry_url})\n",
    "\n",
    "\n",
    "\n",
    "# Create a JSONSerializer instance with your schema\n",
    "json_serializer = JSONSerializer(poem_schema_json_str, obj_to_dict)\n",
    "\n",
    "import time\n",
    "\n",
    "# Create Producer instance\n",
    "# Set up Producer for producing messages\n",
    "producer = Producer({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "# Kafka topic to which you want to send the serialized poem\n",
    "topic_name = 'kreg'\n",
    "\n",
    "# Poem generator\n",
    "poems = poem_generator()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        poem = next(poems)\n",
    "        print(f\"Type: {poem['type']}, Poem: {poem['message']}\")\n",
    "\n",
    "        # Serialize the poem dictionary\n",
    "        serialized_poem = json_serializer(poem, SerializationContext(topic_name, MessageField.VALUE))\n",
    "                    \n",
    "\n",
    "        # Produce the serialized poem to the Kafka topic\n",
    "        producer.produce(topic=topic_name, value=serialized_poem, key=poem['type'], callback=callback_report)\n",
    "        \n",
    "        # Introduce a delay between 1 to 4 seconds\n",
    "        time.sleep(random.uniform(1, 4))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Handle Ctrl+C gracefully\n",
    "    print(\"Stopping Kafka producer.\")\n",
    "finally:\n",
    "    # Flush the producer to ensure all messages are sent\n",
    "    producer.flush()\n",
    "    # Close the Kafka producer\n",
    "    producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned to greg, partition 0\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.json_schema import JSONDeserializer\n",
    "from confluent_kafka.serialization import SerializationContext, MessageField\n",
    "import json\n",
    "\n",
    "# Define your schema string (replace with your actual schema)\n",
    "schema_str = '{\"type\":\"record\",\\\n",
    "                \"name\":\"Poem\",\\\n",
    "                \"fields\":[\\\n",
    "                    {\"name\":\"type\",\"type\":\"string\"},\\\n",
    "                    {\"name\":\"message\",\"type\":\"string\"}\\\n",
    "                ] }'\n",
    "\n",
    "# Define your custom from_dict function\n",
    "def dict_to_obj(msg):\n",
    "    return msg\n",
    "\n",
    "# Create a JSONDeserializer instance with your schema and from_dict function\n",
    "json_deserializer = JSONDeserializer(schema_str, from_dict=dict_to_obj)\n",
    "\n",
    "# Create a DeserializingConsumer instance\n",
    "# Create a SchemaRegistryClient instance (replace schema registry URL)\n",
    "schema_registry_url = \"http://localhost:8081\"\n",
    "schema_registry_client = SchemaRegistryClient({\"url\": schema_registry_url})\n",
    "\n",
    "# Set up Consumer for consuming messages\n",
    "consumer = Consumer({\n",
    "        \"bootstrap.servers\": \"localhost:9092\",\n",
    "        \"group.id\": \"my-group\",\n",
    "        \"auto.offset.reset\": \"earliest\"\n",
    "    })\n",
    "topic_to_read = 'greg'\n",
    "consumer.subscribe([topic_to_read], on_assign=assignment_callback)\n",
    "\n",
    "while True:\n",
    "        try:\n",
    "            event = consumer.poll(1.0)\n",
    "            if event is None:\n",
    "                continue\n",
    "            temp = json_deserializer(event.value(), SerializationContext(topic_to_read, MessageField.VALUE))\n",
    "            if temp is not None:\n",
    "                print(f'Latest temp in {temp.city} is {temp.reading} {temp.unit}.')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.avro import AvroSchema\n",
    "from pyspark.sql.functions import from_avro\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"AvroConsumer\").getOrCreate()\n",
    "\n",
    "# Avro schema for the Poem (reuse from the producer example)\n",
    "poem_avro_schema_str = \"\"\"\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Poem\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"type\", \"type\": \"string\"},\n",
    "    {\"name\": \"message\", \"type\": \"string\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "poem_avro_schema = AvroSchema(poem_avro_schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Kafka topic in Avro format\n",
    "\n",
    "# options\n",
    "#  Use earliest to read all available data from the beginning\n",
    "# Option to handle data loss due to Kafka topic deletion\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"avro_poem_topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "# Deserialize Avro data\n",
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"from_avro(value, '{}') AS poem\".format(poem_avro_schema_str))\n",
    "\n",
    "# Extract fields from the Avro structure\n",
    "df = df.select(\"poem.type\", \"poem.message\")\n",
    "\n",
    "# Output to the console (you can modify this based on your use case)\n",
    "query = df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# Await termination (useful in a real application)\n",
    "query.awaitTermination()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def serialize_poem(poem_dict):\n",
    "    \"\"\"\n",
    "    Serialize a poem dictionary to a JSON-formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    - poem_dict (dict): The dictionary representing a poem.\n",
    "\n",
    "    Returns:\n",
    "    str: JSON-formatted string.\n",
    "    \"\"\"\n",
    "    return json.dumps(poem_dict)\n",
    "\n",
    "def deserialize_poem(json_string):\n",
    "    \"\"\"\n",
    "    Deserialize a JSON-formatted string to a poem dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - json_string (str): JSON-formatted string.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary representing a poem.\n",
    "    \"\"\"\n",
    "    return json.loads(json_string)\n",
    "\n",
    "# Producer Delivery Callback\n",
    "def callback_report(err, event):\n",
    "    \"\"\"\n",
    "    Reports the success or failure of a message delivery.\n",
    "    Args:\n",
    "        err (KafkaError): The error that occurred on None on success.\n",
    "        msg (Message): The message that was produced or failed.\n",
    "    \"\"\"\n",
    "    if err:\n",
    "        print(f'Produce to topic {event.topic()} failed for event: {event.key()}')\n",
    "        return\n",
    "    else:\n",
    "        max_length = 10\n",
    "\n",
    "        val = event.value().decode('utf8')[:max_length] \n",
    "        key = event.key().decode('utf8')\n",
    "        print(f'Messge: {val}, with Key: {key} produced to Topic:{event.topic()}  Part[{event.partition()}] at offset {event.offset()}.')\n",
    "\n",
    "# Assign topic partitions to consumer\n",
    "def assignment_callback(consumer, partitions):\n",
    "    \"\"\" \n",
    "    Callback function invoked when a Kafka consumer is assigned to partitions.\n",
    "    Parameters:\n",
    "    - consumer (confluent_kafka.Consumer): The Kafka consumer instance.\n",
    "    - partitions (list): List of assigned partitions.\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    for p in partitions:\n",
    "        print(f'Assigned to {p.topic}, partition {p.partition}')\n",
    "\n",
    "import avro.schema\n",
    "from avro.io import DatumReader, DatumWriter, BinaryEncoder\n",
    "from io import BytesIO\n",
    "\n",
    "# Avro serialization\n",
    "def serialize_avro(data, schema):\n",
    "    bytes_io = BytesIO()\n",
    "    writer = DatumWriter(schema)\n",
    "    encoder = BinaryEncoder(bytes_io)\n",
    "    writer.write(data, encoder)\n",
    "    avro_serialized_data = bytes_io.getvalue()\n",
    "    # print(f\"Avro Serialized Data: {bytes_io.getvalue()}\")\n",
    "    return avro_serialized_data\n",
    "\n",
    "\n",
    "# Avro deserialization\n",
    "def deserialize_avro(data, schema):\n",
    "    reader = DatumReader(schema)\n",
    "    bytes_io = BytesIO(data)\n",
    "    decoder = BinaryDecoder(bytes_io)\n",
    "    return reader.read(decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "import avro\n",
    "\n",
    "# topic_name = input(\"Enter the topic name to write: \")\n",
    "\n",
    "# Kafka topic name\n",
    "topic_name = \"gavro\" \n",
    "topic_name = \"gjson\" \n",
    "\n",
    "# Poem generator\n",
    "poems = poem_generator()\n",
    "\n",
    "# Set up Producer for producing messages\n",
    "producer_config = {\n",
    "                \"bootstrap.servers\": \"localhost:9092\"\n",
    "}\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "\n",
    "# BASIC SERIALIZATION using json.dumps(poem)\n",
    "\n",
    "# Kafka Procucer\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "\n",
    "# Configure Kafka Producer\n",
    "producer_config = {'bootstrap.servers': 'localhost:9092'}\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "# Poem dictionary\n",
    "poem = {\n",
    "    'type': 'Sonnet',\n",
    "    'message': 'Shall I compare thee to a summer\\'s day?'\n",
    "}\n",
    "\n",
    "# Kafka topic name\n",
    "topic_name = \"greg\"\n",
    "\n",
    "# Serialize and produce the message using JSON \n",
    "serialized_poem = serialize_poem(poem)\n",
    "#serialized_poem = json.dumps(poem)\n",
    "\n",
    "producer.produce(topic_name, key=poem['type'], value=serialized_poem, callback=callback_report)\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Limerick, Poem: Seem early ground history catch prepare.\n",
      "Processing topic: gavro\n",
      "Processing key: \"Limerick\"\n",
      "Processing value: b'\\x10LimerickPSeem early ground history catch prepare.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def produce_function(topic, serialization_option, data):\n",
    "        \n",
    "    # Check if the topic is empty and assign a default value if necessary\n",
    "    # Strip leading and trailing whitespace, assign a default value if empty\n",
    "\n",
    "    if serialization_option == 'n':\n",
    "             \n",
    "        if not topic:\n",
    "            topic_deafult = 'gnone'\n",
    "            topic = topic_deafult\n",
    "            key = data[\"type\"]\n",
    "            value = data[\"message\"]\n",
    "\n",
    "    elif serialization_option == 's':\n",
    "             \n",
    "        if not topic:\n",
    "            topic_deafult = 'gjson'\n",
    "            topic = topic_deafult\n",
    "            key = serialize_poem(data['key'])\n",
    "            value = serialize_poem(data)\n",
    "            \n",
    "    elif serialization_option == 'a':\n",
    " \n",
    "        if not topic:\n",
    "            topic_deafult = 'gavro'\n",
    "            topic = topic_deafult\n",
    "\n",
    "            # Avro schema for the Poem\n",
    "            poem_schema_str = \"\"\"\n",
    "            {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"Poem\",\n",
    "            \"fields\": [\n",
    "                {\"name\": \"type\", \"type\": \"string\"},\n",
    "                {\"name\": \"message\", \"type\": \"string\"}\n",
    "            ]\n",
    "            }\n",
    "            \"\"\"\n",
    "            poem_avro_schema = avro.schema.parse(poem_schema_str)\n",
    "\n",
    "            key = serialize_poem(data)\n",
    "            value = serialize_avro(data, poem_avro_schema)\n",
    "\n",
    "    else:\n",
    "        topic_default = 'None'\n",
    "        print(\"Error\")\n",
    "        return None, None, None\n",
    "    \n",
    "    return topic, key, value\n",
    "\n",
    "# Get user input for serialisation method\n",
    "serialization_option = input(\"Choose serialization option:\\nn - None\\ns - JSON\\na - Avro\\nEnter serialization option: \")\n",
    "\n",
    "# Get user input for the topic name , strip leading and trailing whitespace\n",
    "topic_name = input(\"Enter the topic name (Enter for Default): \").strip()\n",
    "\n",
    "# Poem generator\n",
    "poems = poem_generator()\n",
    "poem = next(poems)\n",
    "print(f\"Type: {poem['type']}, Poem: {poem['message']}\")\n",
    "\n",
    "topic, key, value = produce_function(topic_name, serialization_option, poem)\n",
    "# Your function logic here\n",
    "print(f\"Processing topic: {topic}\")\n",
    "print(f\"Processing key: {key}\")\n",
    "print(f\"Processing value: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_de_function(serialization_option, data):\n",
    "        \n",
    "    # Check if the topic is empty and assign a default value if necessary\n",
    "    # Strip leading and trailing whitespace, assign a default value if empty\n",
    "\n",
    "    if serialization_option == 'n':\n",
    "        key = data[\"type\"]\n",
    "        value = data[\"message\"]\n",
    "\n",
    "    elif serialization_option == 's':\n",
    "        key = deserialize_poem(data['type'])\n",
    "        value = deserialize_poem(data)\n",
    "            \n",
    "    elif serialization_option == 'a':\n",
    "        # Avro schema for the Poem\n",
    "        poem_schema_str = \"\"\"\n",
    "            {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"Poem\",\n",
    "            \"fields\": [\n",
    "                {\"name\": \"type\", \"type\": \"string\"},\n",
    "                {\"name\": \"message\", \"type\": \"string\"}\n",
    "            ]\n",
    "            }\n",
    "            \"\"\"\n",
    "        poem_avro_schema = avro.schema.parse(poem_schema_str)\n",
    "\n",
    "        key = deserialize_poem(data[\"type\"])\n",
    "        value = deserialize_avro(data, poem_avro_schema)\n",
    "\n",
    "    else:\n",
    "        topic_default = 'None'\n",
    "        print(\"Error\")\n",
    "        return None, None\n",
    "    \n",
    "    return key, value\n",
    "\n",
    "# Get user input for deserialisation method\n",
    "deserialization_option = input(\"Choose de-serialization option:\\nn - None\\ns - JSON\\na - Avro\\nEnter serialization option: \")\n",
    "\n",
    "# Get user input for the topic name , strip leading and trailing whitespace\n",
    "topic_name = input(\"Enter the topic name (Enter for Default): \").strip()\n",
    "\n",
    "\n",
    "key, value = produce_de_function(serialization_option, event)\n",
    "# Your function logic here\n",
    "print(f\"Processing topic: {topic}\")\n",
    "print(f\"Processing key: {key}\")\n",
    "print(f\"Processing value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_message(producer, topic, key, value, serialization):\n",
    "\n",
    "    if serialization == 'n':\n",
    "        producer.produce(topic, key=key, value=value)\n",
    "\n",
    "    elif serialization == 's':\n",
    "        serialized_value = json.dumps(value)\n",
    "        producer.produce(topic, key=key, value=serialized_value, value_serializer=lambda v: str(v).encode('utf-8'))\n",
    "        \n",
    "    elif serialization == 'a':\n",
    "        schema = avro.schema.Parse(open('your_avro_schema_file.avsc', 'r').read())\n",
    "        writer = DataFileWriter(open('data.avro', 'wb'), DatumWriter(), schema)\n",
    "        writer.append(value)\n",
    "        writer.close()\n",
    "        avro_bytes = open('data.avro', 'rb').read()\n",
    "        producer.produce(topic, key=key, value=avro_bytes, value_serializer=lambda v: v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
